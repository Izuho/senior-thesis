{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch.multiprocessing as mp\n",
        "import torch.distributed as dist\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from math import sqrt"
      ],
      "metadata": {
        "id": "egMM1KNT6zmP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_process(rank, world_size, layer_size, update_iter, n_qubits, fn, backend='gloo'):\n",
        "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
        "    os.environ['MASTER_PORT'] = '29500'\n",
        "    dist.init_process_group(backend, rank=rank, world_size=world_size)\n",
        "    fn(rank, world_size, layer_size, update_iter, n_qubits)\n",
        "def parallel_train(rank, world_size, layer_size, update_iter, n_qubits):\n",
        "    print('I am ', rank)\n",
        "    train, test, train_size, test_size = data_loader(rank, world_size)\n",
        "    model = FraxClassify(n_qubits, layer_size, world_size)\n",
        "    acc = []\n",
        "    for i in range(update_iter):\n",
        "        model.fit(train=train)\n",
        "        (train_acc, test_acc), (train_score, test_score) = model.eval(train=train, test=test)\n",
        "        if rank == 0:\n",
        "            print(train_acc / train_size, test_acc / test_size, train_score, test_score)\n",
        "def data_loader(rank, world_size):\n",
        "    try:\n",
        "        test_label = torch.from_numpy(np.load('drive/MyDrive/mnist_test_Label.npy'))[0:200]\n",
        "        train_label = torch.from_numpy(np.load('drive/MyDrive/mnist_train_Label.npy'))[0:400]\n",
        "        test_feat = torch.from_numpy(np.load('drive/MyDrive/mnist_test_feat.npy'))[0:200]\n",
        "        train_feat = torch.from_numpy(np.load('drive/MyDrive/mnist_train_feat.npy'))[0:400]\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "    data_len_min = len(train_feat) // world_size\n",
        "    offset = len(train_feat) % world_size\n",
        "    if rank < offset:\n",
        "        start1 = rank*(data_len_min+1)\n",
        "        end1 = start1+data_len_min+1\n",
        "    else:\n",
        "        start1 = offset*(data_len_min+1)+(rank-offset)*data_len_min\n",
        "        end1 = start1+data_len_min\n",
        "    data_len_min = len(test_feat) // world_size\n",
        "    offset = len(test_feat) % world_size\n",
        "    if rank < offset:\n",
        "        start2 = rank*(data_len_min+1)\n",
        "        end2 = start2+data_len_min+1\n",
        "    else:\n",
        "        start2 = offset*(data_len_min+1)+(rank-offset)*data_len_min\n",
        "        end2 = start2+data_len_min\n",
        "    return (train_feat[start1:end1], train_label[start1:end1]), (test_feat[start2:end2], test_label[start2:end2]), train_label.shape[0], test_label.shape[0]\n",
        "def lastbit_Z(state):\n",
        "    return 2 * (torch.norm(state[0:len(state):2])**2) - 1\n",
        "def amplitude_embedding(feat, n, n_qubits):\n",
        "    # feat : torch.tensor of 2^n_qubits elements\n",
        "    if feat.ndim == 1:\n",
        "        feat = feat.reshape(-1,).to(torch.complex64)\n",
        "        feat = torch.repeat_interleave(feat, 2**(n_qubits-n))\n",
        "        feat /= torch.norm(feat)\n",
        "    elif feat.ndim == 2:\n",
        "        feat = feat.reshape(-1,2**n,).to(torch.complex64)\n",
        "        feat = torch.repeat_interleave(feat, 2**(n_qubits-n), dim=1)\n",
        "        feat = feat.transpose(0,1) / torch.norm(feat, dim=1)\n",
        "        feat = feat.transpose(0,1)\n",
        "    return feat\n",
        "def frax_embedding(feat, n, n_qubits):\n",
        "    count = 0\n",
        "    ans = torch.eye(2**n_qubits).to(torch.complex64)\n",
        "    x = 1\n",
        "    assert 2**n % n_qubits == 0, 'Error from frax_embedding!'\n",
        "    for i in range(0, feat.shape[0], 2):\n",
        "        n = torch.zeros(3).to(torch.complex64)\n",
        "        n[0], n[1] = feat[i].to(torch.complex64), feat[i+1].to(torch.complex64)\n",
        "        n[2] = torch.sqrt(1-n[0]**2-n[1]**2)\n",
        "        x = kronecker(x, Frax(n))\n",
        "        if (count+1) % n_qubits == 0:\n",
        "            ans = CZ_layer(n_qubits) @ ans @ x\n",
        "            x = 1\n",
        "        count += 1\n",
        "    return ans[:,0]\n",
        "def kronecker(A, B):\n",
        "    if not isinstance(A, torch.Tensor):\n",
        "        return B\n",
        "    return torch.einsum(\"ab,cd->acbd\", A, B).view(A.size(0)*B.size(0),  A.size(1)*B.size(1))\n",
        "CZ = torch.tensor([\n",
        "    [1,0,0,0],\n",
        "    [0,1,0,0], \n",
        "    [0,0,1,0], \n",
        "    [0,0,0,-1]], dtype=torch.cfloat)\n",
        "def CZ_layer(n_qubits):\n",
        "    if n_qubits == 2:\n",
        "        return CZ\n",
        "    gate1 = CZ\n",
        "    for i in range(2, n_qubits, 2):\n",
        "        if i+1 < n_qubits:\n",
        "            gate1 = kronecker(gate1, CZ)\n",
        "        else:\n",
        "            gate1 = kronecker(gate1, I)\n",
        "    gate2 = CZ\n",
        "    gate2 = kronecker(I, gate2)\n",
        "    for i in range(3, n_qubits, 2):\n",
        "        if i+1 < n_qubits:\n",
        "            gate2 = kronecker(gate2, CZ)\n",
        "        else:\n",
        "            gate2 = kronecker(gate2, I)\n",
        "    return torch.mm(gate2, gate1)\n",
        "X = torch.tensor([[0,1],[1,0]], dtype=torch.complex64)\n",
        "Y = torch.tensor([[0,-1j],[1j,0]], dtype=torch.complex64)\n",
        "Z = torch.tensor([[1,0],[0,-1]], dtype=torch.complex64)\n",
        "XY = (X+Y)/sqrt(2)\n",
        "XZ = (X+Z)/sqrt(2)\n",
        "YZ = (Y+Z)/sqrt(2)\n",
        "I = torch.eye(2, dtype=torch.complex64)\n",
        "def Frax(n):\n",
        "    n = n / torch.norm(n)\n",
        "    return n[0] * X + n[1] * Y + n[2] * Z\n",
        "def Frax_ansatz(n_qubits, param):\n",
        "    # param : torch.Tensor of (n_qubits, 3)\n",
        "    x = 1\n",
        "    for i in range(n_qubits):\n",
        "        x = kronecker(x, Frax(param[i]))\n",
        "    return torch.mm(CZ_layer(n_qubits), x)\n",
        "def replace_Frax_ansatz(n_qubits, measured_qubit, observable, param):\n",
        "    x = 1\n",
        "    for i in range(measured_qubit):\n",
        "        x = kronecker(x, Frax(param[i]))\n",
        "    if observable == 'X':\n",
        "        x = kronecker(x, X)\n",
        "    elif observable == 'Y':\n",
        "        x = kronecker(x, Y)\n",
        "    elif observable == 'Z':\n",
        "        x = kronecker(x, Z)\n",
        "    elif observable == 'XY':\n",
        "        x = kronecker(x, XY)\n",
        "    elif observable == 'XZ':\n",
        "        x = kronecker(x, XZ)\n",
        "    elif observable == 'YZ':\n",
        "        x = kronecker(x, YZ)\n",
        "    for i in range(measured_qubit+1, n_qubits):\n",
        "        x = kronecker(x, Frax(param[i]))\n",
        "    return torch.mm(CZ_layer(n_qubits), x)\n",
        "class FraxClassify():\n",
        "    def __init__(self, n_qubits, layer_size, world_size):\n",
        "        self.n_qubits = n_qubits\n",
        "        self.layer_size = layer_size\n",
        "        self.params = (torch.zeros(layer_size, n_qubits, 3) + 1/sqrt(3)).to(torch.complex64)\n",
        "        self.world_size =world_size\n",
        "    def fit(self, train):\n",
        "        params = self.params\n",
        "        train_feat, train_label = train\n",
        "        for a in range(self.layer_size):\n",
        "            for b in range(self.n_qubits):\n",
        "                R = torch.zeros(3,3)\n",
        "                for c in range(train_feat.shape[0]):\n",
        "                    # y = amplitude_embedding(train_feat[c], 6, self.n_qubits)\n",
        "                    y = frax_embedding(train_feat[c], 6, self.n_qubits)\n",
        "                    for d in range(a):\n",
        "                        y = Frax_ansatz(self.n_qubits, params[d]) @ y\n",
        "                    rx = replace_Frax_ansatz(self.n_qubits, b, 'X', params[a]) @ y\n",
        "                    ry = replace_Frax_ansatz(self.n_qubits, b, 'Y', params[a]) @ y\n",
        "                    rz = replace_Frax_ansatz(self.n_qubits, b, 'Z', params[a]) @ y\n",
        "                    rxy = replace_Frax_ansatz(self.n_qubits, b, 'XY', params[a]) @ y\n",
        "                    rxz = replace_Frax_ansatz(self.n_qubits, b, 'XZ', params[a]) @ y\n",
        "                    ryz = replace_Frax_ansatz(self.n_qubits, b, 'YZ', params[a]) @ y\n",
        "                    for d in range(a+1, self.layer_size):\n",
        "                        rx = Frax_ansatz(self.n_qubits, params[d]) @ rx\n",
        "                        ry = Frax_ansatz(self.n_qubits, params[d]) @ ry       \n",
        "                        rz = Frax_ansatz(self.n_qubits, params[d]) @ rz\n",
        "                        rxy = Frax_ansatz(self.n_qubits, params[d]) @ rxy\n",
        "                        rxz = Frax_ansatz(self.n_qubits, params[d]) @ rxz        \n",
        "                        ryz = Frax_ansatz(self.n_qubits, params[d]) @ ryz                       \n",
        "                    rx = lastbit_Z(rx)\n",
        "                    ry = lastbit_Z(ry)\n",
        "                    rz = lastbit_Z(rz)\n",
        "                    rxy = lastbit_Z(rxy)\n",
        "                    rxz = lastbit_Z(rxz)\n",
        "                    ryz = lastbit_Z(ryz)                       \n",
        "                    R[0,0] += train_label[c] * 2 * rx\n",
        "                    R[0,1] += train_label[c] * (2 * rxy - rx - ry)\n",
        "                    R[0,2] += train_label[c] * (2 * rxz - rx - rz)\n",
        "                    R[1,1] += train_label[c] * 2 * ry\n",
        "                    R[1,2] += train_label[c] * (2 * ryz - ry - rz)\n",
        "                    R[2,2] += train_label[c] * 2 * rz                 \n",
        "                R[1,0] = R[0,1]\n",
        "                R[2,0] = R[0,2]\n",
        "                R[2,1] = R[1,2]\n",
        "                group = dist.new_group(range(self.world_size))\n",
        "                dist.all_reduce(R, op=dist.ReduceOp.SUM, group=group)\n",
        "                eigenvalues, eigenvectors = torch.linalg.eigh(R)\n",
        "                self.params[a, b] = eigenvectors[:, torch.argmax(eigenvalues)]\n",
        "                if dist.get_rank() == 0: print(torch.max(eigenvalues))\n",
        "    def eval(self, train, test):\n",
        "        group = dist.new_group(range(self.world_size))\n",
        "        cri = torch.zeros(4)\n",
        "        train_feat, train_label = train\n",
        "        test_feat, test_label = test\n",
        "        train_size = train_label.shape[0]\n",
        "        test_size = test_label.shape[0]\n",
        "        for a in range(test_size):\n",
        "            # x = amplitude_embedding(test_feat[a], 6, self.n_qubits)\n",
        "            x = frax_embedding(test_feat[a], 6, self.n_qubits)\n",
        "            for b in range(self.layer_size):\n",
        "                x = Frax_ansatz(self.n_qubits, self.params[b]) @ x\n",
        "            if test_label[a] * lastbit_Z(x) > 0:\n",
        "                cri[1] += 1\n",
        "            cri[3] += test_label[a] * lastbit_Z(x)\n",
        "        for a in range(train_size):\n",
        "            # x = amplitude_embedding(train_feat[a], 6, self.n_qubits)\n",
        "            x = frax_embedding(train_feat[a], 6, self.n_qubits)\n",
        "            for b in range(self.layer_size):\n",
        "                x = Frax_ansatz(self.n_qubits, self.params[b]) @ x\n",
        "            if train_label[a] * lastbit_Z(x) > 0:\n",
        "                cri[0] += 1\n",
        "            cri[2] += train_label[a] * lastbit_Z(x)\n",
        "        dist.all_reduce(cri, op=dist.ReduceOp.SUM, group=group)\n",
        "        return (cri[0], cri[1]), (2*cri[2], 2*cri[3])"
      ],
      "metadata": {
        "id": "o6CZ1_im7ZYB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ttt = torch.rand(64)\n",
        "frax_embedding(ttt, 6, 4).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd6NqJ7Faqe8",
        "outputId": "91a257ac-9806-436c-ddaf-9e795f7e29ec"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W = 20 # World_size\n",
        "L = 4 # Layer_size\n",
        "N = 5 # Iteration_size\n",
        "Q = 4 # N_qubits"
      ],
      "metadata": {
        "id": "NYAXKqwM67uW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIkwCDsi6qmJ",
        "outputId": "e88e2565-f9c9-451f-b99e-1e52f94a4626"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am I am I am I am I am I am I am I am I am I am I am   I am I am I am  I am I am    810 1    15 18\n",
            "\n",
            "I am I am I am I am 12    43\n",
            "5719\n",
            "\n",
            "914  2\n",
            "  130\n",
            "\n",
            "\n",
            "6\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "111617\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tensor(89.2169)\n",
            "tensor(143.7787)\n",
            "tensor(147.1383)\n",
            "tensor(153.0700)\n",
            "tensor(153.0700)\n",
            "tensor(154.9816)\n",
            "tensor(155.5312)\n",
            "tensor(158.1523)\n",
            "tensor(158.1523)\n",
            "tensor(158.1523)\n",
            "tensor(163.9559)\n",
            "tensor(164.6218)\n",
            "tensor(164.6219)\n",
            "tensor(164.6219)\n",
            "tensor(164.6219)\n",
            "tensor(170.8678)\n",
            "tensor(0.6600) tensor(0.6750) tensor(170.8678) tensor(100.3787)\n",
            "tensor(171.2578)\n",
            "tensor(172.1441)\n",
            "tensor(173.6735)\n",
            "tensor(177.2885)\n",
            "tensor(177.2884)\n",
            "tensor(177.4182)\n",
            "tensor(177.4635)\n",
            "tensor(177.5072)\n",
            "tensor(177.5072)\n",
            "tensor(177.5073)\n",
            "tensor(177.6192)\n",
            "tensor(177.6660)\n",
            "tensor(177.6659)\n",
            "tensor(177.6659)\n",
            "tensor(177.6660)\n",
            "tensor(177.8346)\n",
            "tensor(0.6825) tensor(0.7000) tensor(177.8345) tensor(102.8941)\n",
            "tensor(177.8391)\n",
            "tensor(178.2924)\n",
            "tensor(179.1773)\n",
            "tensor(179.3338)\n",
            "tensor(179.3338)\n",
            "tensor(179.8779)\n",
            "tensor(180.0022)\n",
            "tensor(180.0184)\n",
            "tensor(180.0184)\n",
            "tensor(180.0183)\n",
            "tensor(180.0643)\n",
            "tensor(180.0938)\n",
            "tensor(180.0938)\n",
            "tensor(180.0938)\n",
            "tensor(180.0938)\n",
            "tensor(180.0954)\n",
            "tensor(0.6800) tensor(0.6950) tensor(180.0954) tensor(105.2347)\n",
            "tensor(180.0955)\n",
            "tensor(180.5196)\n",
            "tensor(180.9533)\n",
            "tensor(180.9733)\n",
            "tensor(180.9734)\n",
            "tensor(181.3430)\n",
            "tensor(181.4469)\n",
            "tensor(181.4531)\n",
            "tensor(181.4532)\n",
            "tensor(181.4532)\n"
          ]
        }
      ],
      "source": [
        "processes = []\n",
        "st = time.time()\n",
        "for rank in range(W):\n",
        "    p = mp.Process(target=init_process, args=(rank, W, L, N, Q, parallel_train))\n",
        "    p.start()\n",
        "    processes.append(p)\n",
        "        \n",
        "for p in processes:\n",
        "    p.join()\n",
        "        \n",
        "print('Implementation time : ', time.time()-st)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6LzGVf7G-noG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}