\section{Kernel method}
\par The problem we're targeting here is classification, one of the machine learning task to predict the label $y_i$ corresponding to the data $\bm{x_i}\in \mathbb{R}^m$, where $y_i \in \{1, -1\}$.
\par Assume the pairs of data and label $\{\bm{x_i, y_i}\}_{i=1,2,\cdots,N}$ where $\bm{x_i}\in \mathbf{R}^m$ is given. In order to improve the analysis performance, we consider nonlinear transformation of input data $\bm{x_i}$ to higher dimensional output $\hat{\bm{x_i}}$ as below.

$$\hat{\bm{x_i}}=\phi (\bm{x_i})\in \mathbf{R}^{\hat{m}}, \hat{m}>m$$

Then, we set the squared error cost function $f$ with parameters $\bm{w}\in\mathbb{R}^{\hat{m}}$ and by modifying these parameters we minimize the cost function. The cost function $f(\bm{w})$ is defined as below:

\begin{equation}
\label{heavy}
\begin{aligned}
f(\bm{w})=&\sum_{i=1}^N (y_i-\hat{\bm{x_i}}^{ \mathrm{T} }\bm{w})^2\\
=& \|\bm{y}-\bm{\hat{X}}\bm{w}\|^2_2
\end{aligned}
\end{equation}

where
$$\bm{y}=\{y_1,y_2,\cdots,y_N\}$$
$$\hat{\bm{X}}=(\hat{\bm{x_1}},\hat{\bm{x_2}},\cdots,\hat{\bm{x_N}})\in\mathbb{R}^{N\times \hat{m}}
$$
When $\hat{m}$ is large, computational cost to minimize the cost function $f$ increases.

\par On the other hand, if we can represent the parameters $\bm{w}$ with a linear combination of $\hat{\bm{x_i}}$, namely
$$\begin{aligned}
\bm{w}=&\sum_{i=1}^N \Tilde{w}\hat{\bm{x_i}}\\
=&\hat{\bm{X}}^{\mathrm{T}}\Tilde{\bm{w}}
\end{aligned}$$
where
$$\Tilde{\bm{w}}=(\Tilde{w_1},\Tilde{w_2},\cdots,\Tilde{w_N})^{\mathrm{T}}\in\mathbb{R}^N$$
then the cost function can be rewritten as below:
\begin{equation}
\label{ker}
\begin{aligned}
f(\Tilde{\bm{w}})&=\|\bm{y}-\hat{\bm{X}}\hat{\bm{X}}^{\mathrm{T}}\Tilde{\bm{w}}\|^2_2\\
&=\|\bm{y}-\bm{K}\Tilde{\bm{w}}\|^2_2\\
\end{aligned}
\end{equation}
where
$$\bm{K}=\hat{\bm{X}}\hat{\bm{X}}^{\mathrm{T}}=
\begin{pmatrix}
\hat{\bm{x_1}}^{\mathrm{T}}\hat{\bm{x_1}} & \hat{\bm{x_1}}^{\mathrm{T}}\hat{\bm{x_2}} & \hat{\bm{x_1}}^{\mathrm{T}}\hat{\bm{x_3}} & \cdots\\
\hat{\bm{x_2}}^{\mathrm{T}}\hat{\bm{x_1}} & \hat{\bm{x_2}}^{\mathrm{T}}\hat{\bm{x_2}} & & \\
\hat{\bm{x_3}}^{\mathrm{T}}\hat{\bm{x_1}} & & & \\
\vdots & & & \\
\end{pmatrix}\in\mathbb{R}^{N \times N}
$$
We write $ij$-th element of $\bm{K}$, namely $k_{ij}$ as $k(\bm{x_i},\bm{x_j})\ (=\hat{\bm{x_i}}^{\mathrm{T}}\hat{\bm{x_j}})$. The function $k$ is called as kernel function and the matrix $\bm{K}$ is called as kernel matrix.
If we can calculate the value of $k(\bm{x_i},\bm{x_j})$ efficiently, then the less computational cost to minimize the cost function \ref{ker} is achieved than \ref{heavy} due to the small dimension of the kernel matrix.

\par In the quantum kernel method, kernel function $k$ is calculated using the quantum circuit like Figure \ref{fig:kernel}. Minimization after calculating the kernel matrix is done on a classical computer.

\begin{figure}[H]
    \centering
    \Qcircuit @C=3.2em @R=1em {
        &&&\lstick{\ket{0}} & \multigate{4}{S(x)} & \multigate{4}{S(x')} & \meter  \\
        &&&\lstick{\ket{0}} & \ghost{S(x)} & \ghost{S(x')} & \meter \\
        &&&\lstick{\ket{0}} & \ghost{S(x)} & \ghost{S(x')} & \meter \\
        &&&\vdots & & &\vdots \\
        &&&\lstick{\ket{0}} & \ghost{S(x)} & \ghost{S(x')} & \meter
    }
    \caption{The circuit used for kernel method.\label{fig:kernel}}
\end{figure}

$S(\bm{x})$ is the unitary operation embedded the data $\bm{x}$ and can be freely defined.  $S(\bm{x'})\dagger$ is the inverse operation of $S(\bm{x'})$. Measurement is done by the measurement projector $|00\cdots0\rangle\langle00\cdots|$, which means the obtained value is the probability of all the qubits being in the state $|0\rangle$. The mathematical definition of kernel function $k$ is the following:
$$
\begin{aligned}
k(\bm{x},\bm{x'})&=\rm{tr} \left(|00\cdots 0\rangle\langle00\cdots 0|S(x')^\dagger S(x)|00\cdots 0\rangle \langle 00\cdots 0| S(x)^\dagger S(x')\right)\\
&=\|\langle 00\cdots 0|S(x')^\dagger S(x)|00\cdots 0\rangle\|^2\\
\end{aligned}$$

\par As seen from the above explanation, measurement of the quantum circuit on the order of $O(N^2)$ is needed with respect to the number of data $N$ to fill the elements of the kernel matrix. 