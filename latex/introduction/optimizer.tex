\subsection{Optimizers}
\par Gradient-based algorithms utilize information from cost function gradients as like many classical machine learning algorithms that already exist. Parameters are basically updated in the direction opposite to the gradient value. The gradient of cost function can be analytically evaluated on quantum computers. There are two ways of evaluation: direct observation and indirect observation. Direct observation uses the auxiliary bit and the gradient is encoded into this bit, which means only one observable is needed \cite{Romero_2019}, while indirect observation can be implemented on the circuit of interest but requires multiple observables \cite{PhysRevResearch.1.013006}.

\par Gradient-free algorithm on the contrary does not need the evaluation of the gradient of the cost function. This algorithm takes advantage of the analytical properties of the cost function landscape. Therefore the design of the cost function is important for gradient-free algorithm. Few methods have been proposed:
Nelder-mead\cite{10.1093/comjnl/7.4.308}, Powell \cite{Pellow_Jarman_2021}, 
Rotosolve / Rotoselect \cite{Nakanishi_2020,Ostaszewski_2021}, Fraxis \cite{fraxis1,Wada_2022}, Quantum analytical descent \cite{koczor}.

\par What is important when using the variational method is that the cost function is tightly minimized and that parameters converge quickly. It is known that in quantum machine learning there can be a problem with the landscape of the cost function called barren plateaus, where the gradient is almost zero everywhere. This occurs especially when the number of parameters is increased in variational method with the gradient-based optimizer. However, gradient-free variational method directly jumps into an optimal solution, which prevents from getting stuck in barren plateaus \cite{book1}. From the convergence speed per iteration, gradient-free methods are also considered to be much faster than the gradient-based methods \cite{TILLY20221}. However, gradient-free optimizers use coordinate descent algorithms which update the parameters one by one, and require multiple (more thtaan three) expectation values to optimize one parameter. Gradient-based optimizers require only two expectation values, and are capable of optimizing all parameters at once. It may be unfair to compare the convergence speed per iteration between gradient-based optimizers and gradient-based optimizers, but several studies nonetheless show that in many cases the gradient-based optimizers converge faster\cite{Ostaszewski_2021,PhysRevResearch.2.043158, https://doi.org/10.48550/arxiv.1912.08660}.
